"Exploring synthesis of flexible neural machines with Zygote.jl"

The video recording is currently at https://www.youtube.com/watch?v=FIeO1yenQ6Y 
starting at 4 hours 58 minutes

Annotated transcript of the talk:

4:58:30

Session chair: So, our next talk today will be about 
exploring synthesis of flexible neural machines
with Zygote, and our presenter is Mishka!


Hi, aha, this [microphone] works, thanks! Thanks for organizers of JuliaCon,
and 

so on my github there are materials related to this presentation:
slides and the open source code and protocols of experiments

and Julia is great when you need to do something non-standard

here we are going to synthesize neural machines

4:59:00

and what will flow through wires is vectors represented by dictionaries
so they are infinite-dimensional, but very sparse vectors
with finite and usually small number of non-zero elements

and this talk has two parts
at first part I'll explain what are these machines
and in the second I explain what I was doing with them for Zygote
and I am really trying to find collaborators on this

4:59:30

so if you transition from traditional neural net to Transformers
the result is much more expressive machines and also machines
which are much easier to train

and so they... and so that's how we get the modern magic

now if you generalize further you can get even more expressive machines
you can have machines

5:00:00

in which you can write general [person - strike that] purpose
continuously deformable dataflow programs and you can have machines
which can easily self-modify, and all kinds of magic,
but they are not optimized for training, they are too flexible,
they completely don't fit any modern workflows

so, Zygote however is very flexible and you can use it to do
at least some training tasks [here] and hopefully 
we'll optimize them [the flexible neural machines] better in the future

so if you think about 

5:00:30

neural computations the essence is that you interleave linear and
non-linear computations, so you just need alternating pattern like this

and so what's the natural degree of generality?
it's not streams of numbers
it's any streams as long as you can combine them with coefficients.

that's ... that's your ... that's what one should aim for

and so you get this very natural class of neural machines
which use 

5:01:00

single neurons [to] transform arbitrary streams
and you also to push power further to the limit you allow
arbitrary arity of activation functions

and you consider streams of tree-like objects
they are very nice to do all kind of interesting things
and you say ok you have unbounded network size,
so network can dynamically change
and so you have your Turing-completeness

5:01:30

because you have unbounded memory finite at any given moment

and so if you look at traditional recurrent nets
these are all numbers and these numbers are linear combinations of these
and neurons process nonlinearly these numbers inside
and we want them to become instead of [numbers we wa]... streams of numbers
we want these to become linear streams

and

5:02:00

so what's interesting about linear streams
essentially if you consider linear combination
that's essentially "artificial attention"
that's what attention is: 
linear combination of high-dimensional vector-like objects

so we are getting a situation where every input of a neuron
becomes [an] attention device

and there are all kinds of these streams
and you can even incorporate

5:02:30

discrete object into them by statistical sum

and here is a typical DMM
it [a neuron] has multiple inputs, multiple outputs
any input is a linear combination of output streams
and inside neurons there can be complicated processing

and it's very powerful, but cumbersome
because these wires have types
this one might be for one kind 

5:03:00

of linear stream
this one might be for another kind of linear stream

and you need this ... and you need this type correctness condition
because if they are of different types then it does not make sense
to connect them

we would like to have one big type which is universal and ...
sufficiently universal,
and [it] turns out that streams of tree-like objects work well

and because they are tree-like objects, they allow us

5:03:30

to define variadic neurons
so we won't need to keep track of arity either

and so we'll call them V-values for vector-like values
and as a homage to S-expressions

and vector-like... and this space... there are several ways
you can look at this space

you can think about it as finite linear combinations... 
formal linear combinations of strings of letters,

or you can think about these as finite prefix trees with
numeric leaves

5:04:00

or you can think about them as tensors of mixed rank:
(sparse scalars)[should be: "scalar"] plus sparse array plus
sparse matrix plus sparse tensor of rank three, for example,
like here, this is the ... an example

so these are all these different ways of looking [at this structure]
I am going to talk about [the] highlighted way
so this is scalar, this is ... on this level is sparse array,
one dimensional,
on this level is sparse matrix,
on this level is sparse tensor of rank three

5:04:30

that['s] how it works

and then this is your neural machine
you take all arguments of a si[ngle neuron]
and unite them in a single tree

so now your activation function just takes a tree and
produces another tree 

but your linear operator works like this:
it computes subtrees on this level [first sublevel]
as linear combination of subtrees on this level [first sublevel]

and that's... 

5:05:00

kind of... it takes some getting used to
and work (for)[with] it, that's a bit unusual,
but that's very simple, easy to implement, nothing to keep track of

and then you can program with this
there are a number of good constructions which help programming
I'll talk about some of them
and the result is that you can do things

for example, [an] important construction is accumulator
which implements memory in a neural machine

5:05:30

so you take a neuron which takes sum of its arguments
and you connect the output to one of these arguments... with weight 1

this ensures that what gets there, stays there,
and then you take updates on the other input.

and that's super-important you use it all the time everywhere

then, because you have arity [of] more than one argument,
it's easy to implement multiplicative construction[s]

5:06:00

you ... one linear combination on one argument, say, gives you a multiplier
and another one gives you what you want to multiply this multiplier by
and you get this ...

and you have sparse vectors of high or infinite dimension

this is [the] example I am going to use in the second part of the talk
consider a neuron which accumulates a count of words in a text

5:06:30

so it will be a map from a token to how many it appears
so that's a vector
and it's an infinite-dimensional space
because number of words is potentially infinite
but it has only [a] finite number of non-zero elements
because the text is finite

that's a typical [situation]

and one can represent all kind of data strcutures
because you have these trees

and then how do ... and then these remarkable things

5:07:00

can easily change themselves
it's a total nightmare for ordinary recurrent neural nets to
change themselves because they have too few neurons
compared to [the] number of weights
but here it's very simple
you just allocate [a] single neuron to hold the whole matrix
of the whole network
and you just hold it in this neuron as an accumulator
and you take update from the other neurons
and then you can modify... [the] network can modify weight[s]

5:07:30

can modify its own topology, can grow

and we did some very innocent things with it [with this mechanism]
we just made [a] network to propagate waves through its own matrix
we just ran some random networks and observed interesting dynamic systems
and we just used it [this mechanism] to do live-coding
we told the network by editor: send it [a] signal:
"modify (itself) [yourself] this way", and it changes while running'

but people thinking more and more

5:08:00

that the ability of AI to ... to change ... to creatively modify itself
in this way and that way holds both big promise and huge peril

and this is the quintessential way 
it's the most clear formalism to do self-modification and self-improvement
so AI safety issues are very real here
and I hope that people who will play with something like this

5:08:30

will mind them and will keep them in mind

because who knows what will happen,
I don't.

*************************

PART 2

OK, so there is this promise. This formalism has nice promise.

if we expect that networks can modify themselves,
they should be able to learn to modify themselves better

and then you can synthesize them
it's... on one hand, you have here small compact nice machines
you can read them and understand them

5:09:00

and that's like program synthesis

but on the other hand, they are... they are defined by matrices
by continuous models, that's like [program inference]

that's a very combination and we hope that it will work
but how to make this hope a reality?

and [the] second part of the talk

Julia to help with flexible things

so Zygote is the first machine learning system with enough flexibility
to do that without distortions

5:09:30

in particular you can take gradients with respect to trees
not with respect to variables which are in arrays or in [matrices]
but you can have variables in [the] leaves of trees and
you can take gradients with respect to them all

now there is some competition
JAX can do it now, and Enzyme can now do it,
but I think Zygote is still... 
for [a] number of reasons I still prefer Zygote for this

so we'll use neural architecture search to synthesize a machine 

5:10:00

with desired properties

consider a machine consisting of modules
which are just our DMM neurons

connect (it)[them] with data pipelines gated by scalars
by DMM weights

and... take a large network and start training it with
sparsifying regularization 
which will press weights... many weights close to zero

and at some point we start

5:10:30

gradually pruning small... small weights
and small connections away while continuing to train

and here so... data pipelines for our task
will carry streams of dictionaries

and we are going to synthesize more or less a feedforward DMM
but with some accumulators so there are some local recurrences still

but basically we start with a feedforward network where every layer
[is] fully connected to every subsequent layer

5:11:00

so with skip connections

and consider handcrafted... this is a handcrafted DMM
which is trying to detect whether there are duplicates
in... in a text, duplicate words or duplicate letters in a string

so here [an] accumulator accumulates this dictionary
of ... mapping words... tokens to their counts

and now you take a norm of this vector and compare it to one

5:11:30

and if it's larger than one then you know there are duplicates
and this difference shows you how much... 
what is the degree of duplication

and here you take "end-of" symbol and take scalar... dot product
with this end symbol and that's how you know how many "end-of" symbols
you will encounter

and so this can stop whenever [the] first non-zero arrives
or it can continue and just compute these counts on the inputs

5:12:00

and now we are going to synthesize... try to synthesize
a DMM which imitates this one

and we form the following feedforward network
[the] initial layer will be these three neurons
[the] final layer will be [the] output [neuron]
and intermediate layers will have 2 neurons of each type
so 8 neurons

and since everything is connected to everything after it
this has about 1500 parameters
and we are going to cut it two orders of magnitude

5:12:30

during training

so... so... the original network had 8 neurons in each layer
and every output of every neuron was connected to every input
of every subsequent neuron
and here we only have left this, because most of the things
are pruned and there (is)[are] only 19 parameters here

but accumulator connections are hard-coded
we are not... we are not... 

5:13:00

we are not touching them

so that's how they compare, the handcrafted one... 
that's how they compare, the handcrafted one and the synthesized one

and you can start looking at this and seeing
the algorithmic interesting things
the process invented
and decide whether they are good or bad
so [it's all] very readable, very analyzable

and

5:13:30

and that's how the matrix looks

this is just a V-value
which is... this is [a] matrix with 19 non-zero elements... weights
and that's [the] first five of them

and the training was done on [an] absolutely minimalistic data set
I took just one string, "test string" with three dots at the end

and I was feeding one character to it every 10 time steps
so one character, 

5:14:00

then 9 empty dictionaries, then another one... very small
training sequence of 140 elements

and still it generalized greatly

it like... when you talk about short strings it works perfectly

but within [the] training envelope
if you start going to long strings, then you start seeing defects

but yeah... but it's impressive that it works so well
I think it's a nice discovery

5:14:30

OK, so that was what we [have] done
now I want to give a couple of slides on concluding remarks
and give [answers to] questions

so if you use Zygote to take this kind of gradients
don't use old style
if you use "params" it won't work

just... it will silently break, and you won't know why

sometimes Zygote generates code which computes gradients incorrectly
it does happen occasionally
do include tests

5:15:00

compare occasionally with numerical derivatives as a sanity check

now... "backprogration theorem" promises us that gradients are fast
no more than 4 times multiplied by [the] number of operations which
the function takes

however this promise in practice is not always held
because very often forward code is well optimizable
and overall code is not so well optimizable
so sometimes slowdown[s] happen and that's unfortunate

5:15:30

and one (have) [has] to deal with it

and finally, it's great to have help from experts
it's like... makes [a] huge difference
and I am very thankful to Mike Innes for unblocking me at one point
it's great!

so this is just an exploration
it's not even a package, it's an open source prototype

yeah... and we would like to make it a technology

and I think it is... 
the scale of this work is such that it needs a small team

5:16:00

people need to think together how to do it, discuss... like

it's not just even coding
it's... people need to like... get together and think 
what is the right approach to make [this happen]

these are very tempting neural machines
it's a shame that we are not using them

so it would be nice to start using them

that's why I am looking [for collaborators]

and I can be contacted via github by opening an issue
or by e-mail

and thank you very much for your attention

5:16:30

and now I am ready for questions

*************************

QUESTIONS

Session chair: Hello... thank you for a great presentation
we have plenty of time for questions
so if anyone wants to ask any, raise your hand and

let's see... I'll just give you the mike now

Question 1: Cool... OK, thanks. 

5:17:00

Did you find in the structures
that you ended up with at the end, are there any... like...
is there anything redundant in there,
or is it the same structure you drew out by hand?

Answer: No, it's different, and it has some quirks,
for example, here we just put input in[to] an accumulator,
but here it for some reason also wants to give it ahead of time
to subsequent elements
and then it wants to... 

5:17:30

it really wants to propagate these things here
and we didn't provide it a [mechanism for this], and instead of just...
and instead of... (preserving)[using] a skip connection
it abused this double-ReLU compare neuron to just ...
basically abused it as a multiplier and did it a few times
so yeah, yeah... there are strange redundancies
and the question is how to get rid of (it)[them] is...
it's basically a starting point for exploration, not an end point

5:18:00

yes... so, it's not optimal...

Question 1: Cool, thank you.

Question 2: Did you see if this infrastructure can debug?
For example, did you take one of your handcrafted models
and break it, and see if the infrastructure can fix it?

Answer: If I... How do you mean, again?

Question 2: So, take your handcrafted model...

Answer: Aha

Question 2: tweak something that it is broken

Answer: OK

Question 2: and then use it as your initial condition...
Can it fix it?

5:18:30

Answer: If I use it as my initial condition?
Then it will... probably, I don't...
But, but... I would expect so, yes...
I think it is easier than to train from the big one
But I did not try, no...

Session chair: Let's see... You have a question?

Question 3: When you see you are taking the derivative
with respect to a tree, how is that different from just
taking the derivative 

5:19:00

with respect to each and every single variable in the tree,
and is just... the tree is just basically a way to index each
variable in the vector?

Answer: Right, right... But if you take derivatives
separately, then it's very long, right? Because then
you multiply... then you basically don't take advantage of
reverse mode and you get [the] number of your variables 
[as complexity factor], so you need to take...

you can probably... 

5:19:30

I thought about reshaping trees dynamically
into arrays, about maintaining translation tables
so that there... so that numbers are stored in flat array[s] and...

and you know, I know how JAX does it, JAX as a compromise
does lists of leaves which are arrays... So it depends...
yeah, I think, I think it is possible to do all kind[s}
of optimizations here...
and I don't know which one to choose... and also...

5:20:00

and also, whatever you choose, 
it would be nice to have a partner or two,
because it is not that trivial [piece of] work...

Question 4: Can you say a bit more about the sparsification?
How does an edge disappear?

Answer: Oh, here is... First of all, my loss function has
an L1 term, so that's [sparsifying]... and then if it's like
if it starts getting small... it is a manual... I do control it
manually,

5:20:30

it is an attended [training] run, it's not blind run,
I do some steps, I see that I want to cut-off [smallest weights]
so it is a probably automatable [one]
I think it is a good project to automate these attended runs
but for now it's like sort of [a] very babysitted run
I did not cheat, everything is recorded, but it's 
it's not... it's not a blind thing.

Session chair: []

5:21:00

Question 5: Yeah, thanks. so I did not completely understand
the whole mechanism, but I am just curious in general.
It seems to me like you are feeding trees to the neural network
which comes with a kind of discrete structure.
So I am just curious about how differentiating over that even
works, what's the definition of [...] here

Answer: Just, it's

5:21:30

it's like, arrays also have discrete structure, it's a grid
of indices, this is just the indices instead of being shaped in
a rectangular regular grid have tree shape. And derivatives [are]
with respect to numbers in the leaves. So it's still derivative
with respect to vector, to coordinates of a vector, but the secondary
structure by which we structure the coordinates is different.

5:22:00

you can like... reshape... you know... you often in machine learning
reshape between one-dimensional and two-dimensional, and here
we can see the natural structure is tree-like and not regular.

Question 5: Um, thanks.

Session chair: Thank you. Let's see. Any other questions anyone wants to ask?
Let's see. Let me see how we are in time. Yeah, we have some more time.
I'll ask you one last question then, you know:

5:22:30

Question 6. I find a lot of these different AD platforms very interesting
and you kind of skim over why you use Zygote versus Enzyme or JAX, which
you know has kind of this tree stuff going on, so...

Answer: Yeah, I can comment. I certainly... JAX is what I started with...
Oh, I started with Zygote and I could not make it differentiate
[with respect to] trees and I could not get help. 

Then I learned to do it in JAX.
But in JAX it's really unpleasant to program.

5:23:00

It's a nasty domain-specific language.
You are not programming in Python, you are programming in JAX, and it's...
Yeah...

But then I have this [the ability to do it in JAX], and I found
a nice issue discussing interface in Zygote, and I said, 
"hey, I can do this in JAX, how can I do this in Zygote", and
Mike Innes himself showed me how to do it, and I saw:
"ha, I am using `params`, and I am stupid, and I should not [use `params`]".

And so that's how he saved me.

And Enzyme, yeah, I worked with William Moses, and he held my hand

5:23:30

but still my experience is, Enzyme is fragile as...yeah... when it falls apart,
it's not like Zygote... when Zygote gives diagnostics, it gives diagnostics...
when Enzyme falls apart, your process just dies, because it usually means
that LLVM code is incorrect. And it's like, you never know [what's going on],
it's ... I don't know, if like... if one can work really close with people
like William then Enzyme might be great... 

5:24:00

but you need to really... because yeah... 
you then generalize, do slightly different [thing],
and something works, and now you do something else, 
and it now breaks... so, it's...

Enzyme is [a] great thing, it's a great product.

But here, you are using non-standard pattern[s],
and they are not tested well.

Session chair: Awesome. Thank you. Well, if we don't have any more
questions, then let's thank our presenter another time, and... yeah...

Presenter: Thanks!

5:24:30

END OF TRANSCRIPT (26 minutes)
