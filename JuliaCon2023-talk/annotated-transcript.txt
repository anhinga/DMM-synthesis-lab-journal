"Exploring synthesis of flexible neural machines with Zygote.jl"

The video recording is currently at https://www.youtube.com/watch?v=FIeO1yenQ6Y 
starting at 4 hours 58 minutes

Annotated transcript of the talk:

4:58:30

Session chair: So, our next talk today will be about 
exploring synthesis of flexible neural machines
with Zygote, and our presenter is Mishka!


Hi, aha, this [microphone] works, thanks! Thanks for organizers of JuliaCon,
and 

so on my github there are materials related to this presentation:
slides and the open source code and protocols of experiments

and Julia is great when you need to do something non-standard

here we are going to synthesize neural machines

4:59:00

and what will flow through wires is vectors represented by dictionaries
so they are infinite-dimensional, but very sparse vectors
with finite and usually small number of non-zero elements

and this talk has two parts
at first part I'll explain what are these machines
and in the second I explain what I was doing with them for Zygote
and I am really trying to find collaborators on this

4:59:30

so if you transition from traditional neural net to Transformers
the result is much more expressive machines and also machines
which are much easier to train

and so they... and so that's how we get the modern magic

now if you generalize further you can get even more expressive machines
you can have machines

5:00:00

in which you can write general [person - strike that] purpose
continuously deformable dataflow programs and you can have machines
which can easily self-modify, and all kinds of magic,
but they are not optimized for training, they are too flexible,
they completely don't fit any modern workflows

so, Zygote however is very flexible and you can use it to do
at least some training tasks [here] and hopefully 
we'll optimize them [the flexible neural machines] better in the future

so if you think about 

5:00:30

neural computations the essence is that you interleave linear and
non-linear computations, so you just need alternating pattern like this

and so what's the natural degree of generality?
it's not streams of numbers
it's any streams as long as you can combine them with coefficients.

that's ... that's your ... that's what one should aim for

and so you get this very natural class of neural machines
which use 

5:01:00

single neurons [to] transform arbitrary streams
and you also to push power further to the limit you allow
arbitrary arity of activation functions

and you consider streams of tree-like objects
they are very nice to do all kind of interesting things
and you say ok you have unbounded network size,
so network can dynamically change
and so you have your Turing-completeness

5:01:30

because you have unbounded memory finite at any given moment

and so if you look at traditional recurrent nets
these are all numbers and these numbers are linear combinations of these
and neurons process nonlinearly these numbers inside
and we want them to become instead of [numbers we wa]... streams of numbers
we want these to become linear streams

and

5:02:00

so what's interesting about linear streams
essentially if you consider linear combination
that's essentially "artificial attention"
that's what attention is: 
linear combination of high-dimensional vector-like objects

so we are getting a situation where every input of a neuron
becomes [an] attention device

and there are all kinds of these streams
and you can even incorporate

5:02:30

discrete object into them by statistical sum

and here is a typical DMM
it [a neuron] has multiple inputs, multiple outputs
any input is a linear combination of output streams
and inside neurons there can be complicated processing

and it's very powerful, but cumbersome
because these wires have types
this one might be for one kind 

5:03:00

of linear stream
this one might be for another kind of linear stream

and you need this ... and you need this type correctness condition
because if they are of different types then it does not make sense
to connect them

we would like to have one big type which is universal and ...
sufficiently universal,
and [it] turns out that streams of tree-like objects work well

and because they are tree-like objects, they allow us

5:03:30

to define variadic neurons
so we won't need to keep track of arity either

and so we'll call them V-values for vector-like values
and as a homage to S-expressions

and vector-like... and this space... there are several ways
you can look at this space

you can think about it as finite linear combinations... 
formal linear combinations of strings of letters,

or you can think about these as finite prefix trees with
numeric leaves

5:04:00

or you can think about them as tensors of mixed rank:
(sparse scalars)[should be: "scalar"] plus sparse array plus
sparse matrix plus sparse tensor of rank three, for example,
like here, this is the ... an example

so these are all these different ways of looking [at this structure]
I am going to talk about [the] highlighted way
so this is scalar, this is ... on this level is sparse array,
one dimensional,
on this level is sparse matrix,
on this level is sparse tensor of rank three

5:04:30

that['s] how it works

and then this is your neural machine
you take all arguments of a si[ngle neuron]
and unite them in a single tree

so now your activation function just takes a tree and
produces another tree 

but your linear operator works like this:
it computes subtrees on this level [first sublevel]
as linear combination of subtrees on this level [first sublevel]

and that's... 

5:05:00

kind of... it takes some getting used to
and work (for)[with] it, that's a bit unusual,
but that's very simple, easy to implement, nothing to keep track of

and then you can program with this
there are a number of good constructions which help programming
I'll talk about some of them
and the result is that you can do things

for example, [an] important construction is accumulator
which implements memory in a neural machine

5:05:30

so you take a neuron which takes sum of its arguments
and you connect the output to one of these arguments... with weight 1

this ensures that what gets there, stays there,
and then you take updates on the other input.

and that's super-important you use it all the time everywhere

then, because you have arity [of] more than one argument,
it's easy to implement multiplicative construction[s]

5:06:00

you ... one linear combination on one argument, say, gives you a multiplier
and another one gives you what you want to multiply this multiplier by
and you get this ...

and you have sparse vectors of high or infinite dimension

this is [the] example I am going to use in the second part of the talk
consider a neuron which accumulates a count of words in a text

5:06:30

so it will be a map from a token to how many it appears
so that's a vector
and it's an infinite-dimensional space
because number of words is potentially infinite
but it has only [a] finite number of non-zero elements
because the text is finite

that's a typical [situation]

and one can represent all kind of data strcutures
because you have these trees

and then how do ... and then these remarkable things

5:07:00

can easily change themselves
it's a total nightmare for ordinary recurrent neural nets to
change themselves because they have too few neurons
compared to [the] number of weights
but here it's very simple
you just allocate [a] single neuron to hold the whole matrix
of the whole network
and you just hold it in this neuron as an accumulator
and you take update from the other neurons
and then you can modify... [the] network can modify weight[s]

5:07:30

can modify its own topology, can grow

and we did some very innocent things with it [with this mechanism]
we just made [a] network to propagate waves through its own matrix
we just ran some random networks and observed interesting dynamic systems
and we just used it [this mechanism] to do live-coding
we told the network by editor: send it [a] signal:
"modify (itself) [yourself] this way", and it changes while running'

but people thinking more and more

5:08:00

that the ability of AI to ... to change ... to creatively modify itself
in this way and that way holds both big promise and huge peril

and this is the quintessential way 
it's the most clear formalism to do self-modification and self-improvement
so AI safety issues are very real here
and I hope that people who will play with something like this

5:08:30

will mind them and will keep them in mind

because who knows what will happen,
I don't.

*************************

TRANSCRIPT IN PROGRESS (the above covers first 10 minutes)
